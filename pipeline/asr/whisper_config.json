{
  "model": {
    "name": "base",
    "language": "en",
    "task": "transcribe"
  },
  "training": {
    "batch_size": 16,
    "gradient_accumulation_steps": 2,
    "learning_rate": 1e-5,
    "warmup_steps": 500,
    "max_steps": 5000,
    "eval_steps": 500,
    "save_steps": 1000,
    "logging_steps": 100,
    "fp16": true,
    "gradient_checkpointing": true
  },
  "data": {
    "train_manifest": "data/whisper_dataset/train.jsonl",
    "val_manifest": "data/whisper_dataset/val.jsonl",
    "test_manifest": "data/whisper_dataset/test.jsonl",
    "audio_sample_rate": 16000,
    "max_audio_length": 30.0,
    "preprocessing": {
      "normalize": true,
      "augmentation": false
    }
  },
  "optimizer": {
    "type": "AdamW",
    "weight_decay": 0.01,
    "adam_beta1": 0.9,
    "adam_beta2": 0.999,
    "adam_epsilon": 1e-8
  },
  "scheduler": {
    "type": "linear",
    "num_warmup_steps": 500
  },
  "output": {
    "output_dir": "models/asr/whisper_finetuned",
    "logging_dir": "logs/whisper_training",
    "save_total_limit": 3,
    "load_best_model_at_end": true,
    "metric_for_best_model": "wer",
    "greater_is_better": false
  },
  "evaluation": {
    "metrics": ["wer", "cer"],
    "prediction_loss_only": false
  },
  "hardware": {
    "device": "cuda",
    "num_workers": 4,
    "pin_memory": true
  },
  "notes": "Configuration for Whisper base model fine-tuning on Indian English data. Adjust batch_size based on GPU memory. For 8GB GPU, use batch_size=8 or lower."
}

